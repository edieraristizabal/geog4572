[
  {
    "id": 1,
    "title": "How Not To Be a Minority Person",
    "content": "It is said that Winston Churchill claimed to have Red Indian blood through one of his American ancestors. If true, it is strange that he should make the claim, for it is as if being a descendent of the Duke of Marlborough was not enough and that he needed the genes of an Indian chief for added luster. If perchance Churchill had Chinese blood, I doubt he would have boasted about it, for to him and his fellow aristocrats a Chinaman was a coolie or someone who sweated in a laundry shop. My point is that not all ethnic minorities are looked down upon. Some are admired for certain qualities, outstandingly, that of courage, which Europeans see in American Indian warriors. If I had never left China, I would be a member of the majority, with the confidence that such membership brings. But, given my affectional orientation, I would also be relegated to a more or less rejected group. As for my status in America, I am a minority person three times over: a Chinese in a country where whites dominate, a fellow traveler of Christianity in a university that is aggressively secular, and a man who finds male clumsy gentleness more touching than female delicate concision. The result is that I often feel isolated and, at times, despairingly alone, a condition that cannot be assuaged by simply joining a group. In fact, joining a group can exacerbate the feeling. My problem, I have come to realize, is that I am unique. But, then, so is everyone else! Uniqueness of individuals is a species characteristic, one that distinguishes us from other animals. Our sense of belonging, of being wholly part of a group, is more illusory than real. Consider the gathering in this room. All the chairs in it are identical, you are all UW students, you are all young and you all look much alike by virtue of your youth, and you are all listening to me. We are a community and almost one body. Yet, I would be bewildered–and probably shocked—if I were able to look into your minds. We dread our uniqueness. We seek to belong. Belonging to a group makes the individual feel less vulnerable and more self-confident. As for the group, its collective sense of self is typically one of confidence, provided it is isolated from more advanced neighbors. There were many such small, isolated groups a hundred years ago. Ethnographers of the time labeled them \"primitive\" on account of the simplicity of their material culture, but the people themselves had a more flattering view of their standing in the world. Given the fact that each time they moved away from their home base, they saw fewer and fewer people until only wilderness remained,  they could believe that they occupied the world's geographical, population, and cultural center. That belief was shattered when, from the 1800s on, they encountered Europeans in increasingly large numbers. Demoralization followed. Tribal groups that once took their centrality for granted could see that they were not at the center, but at the margins. As a result, their status in their own eyes dropped from being a people of the center, or cosmopolites, to being a people of the margins, or ethnics.Ever since, the challenge for ethnics is to regain their former sense of centrality and self-confidence. One widely adopted solution is to withdraw into their own culture. Ethnic leaders promote this withdrawal, which is understandable enough, but so do affluent whites. Their support, though outwardly well-intentioned, is somewhat suspect. Why? Because it contains an element of self-interest. After all, ethnic cultures are an amenity for white people who, on vacation, can enjoy the exotic foods, costumes, and dances that ethnics provide. Now, as an ethnic myself, I ask, \"Do we want to be tourist attractions? Do we want to be gawked at? Is returning to our roots the best way to regain our self-confidence and win esteem from the larger world?\"China offers a different answer to the problem of marginalization. China, of course, was not a small country even two thousand years ago. But it has something in common with the small  groups that I mentioned earlier. Like them, though with better justification, the Chinese saw themselves as located at the world's geographical, population, and cultural center. Every encounter they had with an alien people gave them further proof of their centrality and superiority: that is, until the 1830s and 40s when they suffered a succession of humiliating defeats at the hand of Europeans. The Chinese quickly saw that the European machinery of war, backed by science and technology, gave them a huge advantage. How to respond? One response was to withdraw into Chinese culture. After all, China had been a flourishing empire for thousands of years. But, in the end, this response was rejected as hopelessly retrograde. Instead, the Chinese decided to follow the West's scientific and technological lead. Their rationale went something like this: \"If we are good and wise as we have always claimed to be and if we merit the title we have given ourselves, namely, the Middle Kingdom, then nothing that is true and good can be alien to us, not a part of our heritage.\" As a further face-saving device, the Chinese used the word \"modern\" rather than \"Western\" to imply that progress is a matter of time rather than of the creativity of a particular people and place.Now, let me turn from group to individual self-confidence–to me. Although I am a minority person in more than one sense, I nevertheless manage to feel central, not marginal. How come? My Mandarin family background is no doubt partly responsible in this reassuring feeling, but it is by no means the only cause. Another is psychological, namely, my strong desire since childhood  to know the Good and human virtue. Knowing the Good and being virtuous just seemed to me matters of unchallengeable importance, and so to be engaged with them was to be central. In any case, given the strength of my desire, it could never have occurred to me to restrict my search to my own culture. Indeed, some deep-seated dissatisfaction with my own culture may well have prompted  the wider search in the first place.By goodness or the Good, I have in mind an order of achievement that goes beyond the normal and the natural. To me, the human species itself is abnormal or unnatural. We say over and over again that we are a part of nature, but even to say it separates us from nature for no other natural being makes any such claim. But there is no need to appeal to logic. We have only to look around us to be convinced.  Living close to the ground is natural, but above the cloud on the eightieth floor of Chicago's Hancock building? Communicating with body language and the spoken word is natural, but with Twitter and Facebook? Knowing the local geography is natural and practical, but the universe in its outermost reaches? Caring for kin and neighbor is natural–the chimpanzees do it too",
    "time": "2011"
  },
  {
    "id": 2,
    "title": "Surviving Death",
    "content": "I’ve just finished reading a book called Surviving Death, a 393-page work written by Mark Johnston of Princeton University. I was astonished that an academic philosopher of distinction could write a book with such a title. He himself was teased by his colleagues when he confessed his project to them. What is Johnston’s position? Can he seriously believe in an afterlife, and if he does, what is the nature of that afterlife? I can’t hope to do justice to Johnston’s dense and subtle arguments. What follows, then, is my own limited understanding, my own extrapolations and--dare I say, insights?Johnston’s position lies somewhere between two extremes that are commonly accepted in our time. One is the belief in bodily resurrection. On Judgment Day, the good will rise to Heaven, the bad will descend to Hell. For centuries, Christians have subscribed to this belief and many, the fundamentalists, still do. The other extreme, accepted by liberal secularists, is that our body will decay and become dust, but we will be remembered and, moreover, the consequences of our good deeds and fertile thoughts will linger on. You probably are not happy with either extreme, inclined to dismiss the one as pure superstition and the other as feeble consolation. If so, you have Johnston’s sympathy. He, for his part, argues for a vision of afterlife that is weaker than the one, that is to say, less materialistic and embodied, and stronger than the other, that is to say, more embodied than just fading memories and impacts. An incarnate afterlife is not, however, available to everyone. Only the good have access. This caveat appeals to me, for I have always wanted goodness to count seriously, and it seems to me that it cannot do so unless it has some sort of metaphysical grounding. That would seem to be Johnston’s position too. His argument for an incarnate afterlife is also a defense of the seriousness–the deep value–of goodness. Unfortunately, I am not bright enough to follow the intricacies of his argument. What I do have from reading him is a convincing psychology.Good people are by definition selfless. Not being burdened by a tiresome self forever demanding attention and massaging, good people devote their mind and love to the outside world. Scientists can be selfless, so engrossed with nature’s mysteries that they forget sleep, even food. They dwell in nature’s mysteries, and are so de-centered as to become incarnate in the objects they study. When a student asked George Wald, a Nobel laureate in chemistry, how will he–the student–know that he has truly understood his subject, Wald’s reply was, “Do you know how it feels to be a molecule?” Now, extend this power of empathy to animate nature, and you have a more conventional picture of the good man or woman. Take the Samaritan, an archetype of goodness in the moral realm. He sees a wounded stranger by the roadside and he, for the moment, lives in that stranger, becomes that stranger, is incarnate or embodied in that stranger. He feels the stranger’s sense of abandonment, and immediately proceeds to help. He can no more step aside than he can refrain from drinking water when he is desperately thirsty. The “I” and the “he” are no longer separate. They have become one. The same psychology is at work when the good person identifies with the beauties of nature and of the world. He is not only a spectator. He not only watches and admires that which is before him, but has momentarily become it. He has put on the glory of the sunset, the majesty of a waterfall, the innocence of a baby, the courage of a firefighter, the cozy hospitality of a cottage, the pride of a skyscraper.If you follow this line of thought, you will see that the good person cannot fear death. Death is feared for a variety of reasons, but at their root is the cessation of self, the end of a particular individual–ME! The good person is, however, someone who dies to self everyday so that the final cessation is no big deal. He dies to self not from morbidity or self hatred, but from an overflowing energy and the love of life. His own self is, to him, too confining. It is not that the good person lives in others, which can be a swelling of self into other people’s lives, with the dire result–whether consciously or subconsciously sought–of domination. Rather it is his delight in certain forms of virtue or goodness. Finding them too restricted by his own body and mind, the good person seeks their existence, both full blown and in kernel form, in other people, rejoicing and admiring where the goodness is ripe and nurturing where it is as yet only a promise. To the good person, the forms of virtue or goodness are of such importance that whether they are incarnate in him or in some other body fades into insignificance.All right, you say, but at the end of the day, you still find what I have just said unconvincing, not so much from the defects of my reasoning as from the depth of your own personal experience. From such experience, you conclude, as I confess I often do myself, that this tangible, individual life that we call our own is all that is real, all that matters. Whatever else comes after is floss. But aren’t we deluded when we give that solidity–that reality–to our daily existence? Let me explain, using myself as an example. I feel very solid and real as I type this letter. You, my readers, are just shadows. On the other hand, what about the “me” of yesterday? What kind of a human being was I only twenty-four hours ago? I recall that I had cornflakes for breakfast, Sichuan chicken for lunch, and Ramen noodles for dinner. I read a book and answered my e-mails. I chatted pleasantly with friends. The funny thing is that I don’t remember my yesterday’s self as “I”, but as the third person singular “he.” And this “he” I don’t consider at all important, not only from an impersonal point of view, but also from the viewpoint of the “I” who now is busily typing a letter. So what is the loss to me when this “I” eventually dies, and all the virtues and qualities I value–the best part of me, the part that feels most real–literally live on and flourish in other people?Good people survive death. Most of us–the moderately good–do so, too, though less solidly. The bad don’t. They don’t because they lack the ability to dwell in others; they are unable to see the way others see and feel the way others feel. “How does it feel to be a molecule?” The bad may be accomplished in abstract thought, but are unlikely to know how a molecule feels. Given this deficiency, even in science, they fail to reach the highest level of achievement. The ability to put oneself in the position of another makes one more objective, more impersonal. The bad are too sunk in the self and its needs to be objective and impersonal. When a bad person dies, the bad consequences of his deeds may live on. He is, however, only their cause. He can hardly be said to live on in the lives he has ruined. In the final analysis, bad people are mortal because subjectivity is mortal: it ends with the demise of the subject.Now, let me return to the good person, provisionally defined as one who lives in others and for others, and beyond human beings and human reality, in the moth that circles the candle flame and in the overwhelming splendor of the aurora borealis. But the good person also lives in himself and for himself. He is kind to himself, thoughtful of himself, and nurtures what is best in his own being. But isn’t this selfishness? No, because the good person, in his impersonality, sees his own body and mind as the other, as no less the other than the abandoned man by the roadside who stirs the compassion of the good Samaritan. The good person is the good Samaritan even to himself. He is, after all, his nearest neighbor. He therefore fulfills his obligation and does all he can for himself, but, like the good Samaritan, he then moves on to other needs and deeds, to other virtues, to creation and its God–and seldom looks back.",
    "time": "2011"
  },
  {
    "id": 3,
    "title": "Gary Locke and his Backpack",
    "content": "A summer of discontent--economic woes made worse by political deadlock–prompts the world to wonder whether America is indeed in decline. Economic woes, however serious, are solvable if there is political will, backed by a political culture that is fundamentally sound. The sight of politicians, in their ignorance and arrogance, damning their opponents in crude language,  turning “compromise” into a dirty word rather than a willingness to listen to the other, and then to see such speeches received with standing applause is a shock to America’s friends and a delight to America’s rivals.China is a rising power. Its leaders–and maybe its people too–as they watch the tacky political theater in Washington DC and Iowa may think, well, democracy is not all good and authoritarianism is not all bad. That is, until August 17, 2011, when a businessman in Seattle’s airport captured the image of a man carrying a backpack and buying coffee at Starbucks, and within minutes, that image spread through internet China and made the Chinese wonder again, “Hey, maybe America does have something we sorely lack, namely, humility in a nation’s leaders.” This humility is so deeply ingrained that the man carrying the backpack wasn’t the least conscious that he was being humble. Who is this man? He is Gary Locke, newly appointed American Ambassador to China. In China, even the lowliest official stands on his dignity, will have an underling carry his bag and bring him coffee. Locke, however, is no lowly official: in the diplomatic world, his full title is no less than His Excellency, the American ambassador Extraordinary and Plenipotentiary! Arriving at Beijing airport, the ambassador and his family slipped into a minivan rather than use the official limousine. All of this is quite normal from Locke’s point of view. After all, his boss, the President of the United States, carried his own umbrella in a shower that greeted him as he emerged from his airplane in Shanghai.Political culture is everywhere rooted in religion. That of the Western world is rooted in Christianity. In all religions, God–that is, power–is to be worshiped. The traditional posture of the worshiper is kneeling. One kneels to Power. Christianity is unique in that it is God who kneels; God who bends low to wash the feet of his disciples. Christ’s example has not undermined the class structure of Christendom. Far from it. Nor has it destroyed the arrogance of power–alas! Nevertheless, it has implanted a core belief–lukewarm but never totally cold–that says, “all human beings are made in the image of God, therefore, all are au fond equal.” Consider Louis XIV. Who is more an embodiment of absolute power than he, the Sun King? Yet, he takes his hat off to the maid when he happens to run into her in a corridor of Versailles. He is deferring to his equal in the eyes of God. Madame de Maintenon deems her sister-in-law unspeakably vulgar because she fails to thank the footman who performs a service. Remember Marie Antoinette?  She is so out of touch with reality that, when told that the people have no bread, she is reported to have said, “Let them eat cake.” Bad, very bad. But the following story makes me think that even she, unknown to herself, is affected by a sense of fundamental equality. The Queen sits for her portrait. The young artist is nervous and drops his brush. The Queen picks it up and hands it back to him. What? Is that all? But my point is. That small gesture of picking up the brush is utterly inconceivable in imperial China. Can you imagine the Dowager Empress Cixi doing that for her portraitist? He is lucky if he escapes with his head intact.Now, my worry is this. With the dimming of Christianity and its radical social values, will American officials eventually become as standoffish, as insistent on the perks of office, as officials in other cultures and civilizations, including the Chinese? In the United States, politicians on the stump still carry their own bags. But for how long? Arrogance is indivisible. To be so full of oneself as to refuse to countenance an opponent’s viewpoint  is political arrogance, but one that easily transmutes into arrogance in social manners–that is to say, in everyday human relationships. When I feel depressed by the present political culture in America, I cheer myself up with the image of Gary Locke at Starbucks. There has to be something right in a political culture that puts an African-American in the White house and has the son of Chinese immigrants be America’s ambassador to China!",
    "time": "2011"
  },
  {
    "id": 3,
    "title": "Classical Music",
    "content": "Alex Ross and Anthony Tommasini are, respectively, music critics of The New Yorker and The New York Times. They are friends, but are not quite on the same wavelength regarding classical music. Ross notes that, to the general public of our time, classical music has the smell of moth balls and that the very word “classical” is redolent of the past, a snobbish past in which ladies and gentlemen wish to be seen in their finery and admired for their cultural taste rather than to listen. Also in the same symphony hall are snobs of another kind, detectable by the music score spread over their knees, their eyebrows knitted in deep concentration, except when angry glances are thrown at a neighbor who can’t quite suppress a cough. And, then, there is the third type of snob, young and very well educated. They, in their sophistication, are reluctant to acknowledge their CDs of Bach and Mozart, but are happy to boast their library of jazz and the Blues, W.C. Handy and Louis Armstrong, Bob Dylan and the Beatles, as well as the latest in Punk Rock and Heavy Metal. Their one shame, musically speaking, is that in their adolescence they fell briefly under the spell of Beethoven’s Eroica Symphony.   Ross himself was under the exclusive spell of classical music until age 20, when he suddenly found new life and new joy in music that takes place in less formal settings–certainly not ones with chandeliers overhead–in which the audience, overwhelmingly young, are rapturously engaged with the throbbing sound of gyrating performers on stage. In Ross’s book Listen To This (2010), his comments on classical music are always detailed, using musical notations to illustrate a fine point in a particular composition or a segment of a composition. And he can be wholly admiring. For example, taking advantage of technology, he put Mozart’s all twenty-seven renditions of the Kyrie on the computer. Listening to them one by one, he was astonished by Mozart’s inexhaustible inventiveness: his Kyries “range from the ravishingly sweet to the forbiddingly severe, each a convincing simulacrum of the power of the Lord.” Why doesn’t Ross treat the music of Bob Dylan and the Beatles, which he calls the high art of our time, with the same respect–that is to say, with the same detailed analysis bar by bar, and the same attention to the work’s overall structure? Ross’s appreciation seems to be more about how Dylan or the Beatles perform, as though they are performing artists rather than composers. Another trait of reviewers of popular music, including Ross, is that they pay as much attention to the lyrics as to the music itself. By contrast, no reviewer of Schubert’s Die Schöne Müllerin will let the music play second fiddle to the words.   A characteristic of classical music, distinct from music of earlier times and modern popular music, is its exceptional range of pitch and loudness. In the same classical piece, one may hear notes that soar into the sky and notes that hug the earth, notes so soft as to be barely audible and notes so loud as to be deafening. Corresponding to this range of sounds is a range of moods that can vary from tenderness to anger, from withdrawal to triumphalism. A combination of “dove” and “crocodile” was how a critic of Beethoven’s time described Beethoven’s Eroica Symphony. What I miss from popular music are the extremes. Since much of it is vocal, accompanied by the guitar, both of which have limited range, this is hardly surprising. But why my emphasis on mood? Well, as I see it, the range, subtlety, and force of how we feel toward nature and other people defines our humanity; and no human creation does a better job of expressing them, better even than the pictorial art of Ma Lin and the verbal art of Shakespeare, than certain compositions of the classical masters.   Anthony Tommasini of the New York Times  recently attempted something in his music column that his counterpart in the New Yorker is unlikely to indulge–unlikely because Ross is too cautious and perhaps also too sophisticated. Tommasini ranked the top 10 classical composers, omitting only the living. They are–in order of greatness--Bach, Beethoven, Mozart, Schubert, Debussy, Stravinsky, Brahms, Verdi, Wagner, and Bartok. Readers of his column quickly offered disagreement, especially at the lower end of the list. Why Bartok rather than Mahler or Bruckner? Tommasisni himself wavered only in his choice of number two. Mozart or Beethoven? He handed the honor to Beethoven largely on the basis of an epiphany about Beethoven during the early 1980s when he heard Leon Kirchner conduct the Harvard Chamber Orchestra. Kirchner began with the Piston symphony, a work of the 1950s, followed by Debussy’s “La Mer,” completed in 1905, but somehow made the Debussy sound more modern than the Piston. “After intermission, Peter Serkin joined Kirchner for a performance of Beethoven’s Fourth Piano Concerto that brought out the mysticism, poetic reverie, and wildness of the music. The Beethoven sounded like the most radical music in the program by far: unfathomable and amazing” (The New York Times, January 23, 2011).   Will any popular art of our time be as timeless? Will anyone, two hundred years from now, still regard Ellington’s “Ebony Rhapsody,” the Beatles’s “Yellow Submarine,” or Dylan’s “Blood on the Tracks,” as radical, unfathomable, and amazing? I don’t like the word “low,” as in “highbrow and lowbrow” art–the expression is officious and dismissive--but surely there are grounds for saying of some musical compositions that they are “great” and of others that they are “good” or “popular”?",
    "time": "2011"
  },
  {
    "id": 4,
    "title": "The Child as Artist and Thinker",
    "content": "Up to age seven or eight, the child is an artist, a philosophical thinker, and possibly both. As artist, she draws and paints, tells fantastic stories, and in her use of language quite often comes up with original metaphors, talents that if they continue to develop may mean that one day she will be a Mary Cassatt, a Charlotte Bronte, or an Emily Dickinson. Such artistic creativity is often noted. Less often noted is the child as philosophical thinker. A five-year-old boy says to his mother, “Isn’t it strange that if a zero comes at the end of the number it means a lot, but if it is at the beginning of a number it doesn’t mean anything at all?” Predictably, this boy grew up to be Hans Bethe, a distinguished physicist. Here are some other examples of precocity that I have selected from Gareth Matthew’s Philosophy and the Young Child (Harvard, 1980).Tim (six years), while busily engaged in licking a pot, asks, “Papa, how can we be sure that everything is not a dream?” Note that he doesn’t ask, “Am I dreaming?” but rather whether everything is a dream, including the pot he is licking. What an extraordinary ability to disengage from context this question shows!David (five years) worries about whether an apple is alive. He decides that it is when it’s on the ground but not when it has been brought into the house. His reasoning seems to be that, on the ground, the apple is still a part of its natural cycle of life, but in the house it is not. Not being a part of that natural cycle, it is dead and hence can be eaten without compunction.Gareth Matthew is tucking his eight-year-son, John, in bed. The boy looks up and asks, quite without warning, “Daddy, why don’t I see you double, because I have two eyes and I can see you with each one by itself?”.“Why do I have two names?” a young child asks. “Is it because I can lose one?” “Don’t be silly,” says another child, “your can’t lose a name.” “But why not? I can forget it, and isn’t that as good as losing?”A little girl of nine asks, “Daddy, is there really God?” The father answers that it isn’t very certain, to which the child retorts, “There must be really, because he has a name!” [If God is not and cannot be named, as in Judaism, then the little girl might argue that he cannot exist].Ian (six years) finds to his chagrin that the three children of his parents’ friends are monopolizing the television, preventing him from watching his favorite program. “Mother,” he asks in frustration, “why is it better for three people to be selfish than for one?”[Here is an account that should appeal to geographers]. Seven-year-old Michael and his friends have just had a Narnia story by C. S. Lewis read to them. The discussion that follows, led by a grown-up, begins with the universe. Michael knows that his father has written a paper on finite models for the universe. He is pleased because he doesn’t like one that is unbounded. The idea of an infinite universe, he says, produces a funny feeling in his stomach. The discussion then turns to death. Michael says firmly, “It is more important to have maps and know where you are than what happens after you die.” To Michael, death seems to call up an image of unmarked and unbounded space.Why does young children’s genius for art and thought begin to fade after a certain age? One reason is that by then they start to yearn for their peers’ social acceptance. To ensure it, they sacrifice their colorful vocabulary, their high imagination, and their penchant to ask difficult and impracticable questions. Sociality may be necessary to group formation and survival, but unfortunately it adversely affects imagination and intelligence. Conversation, even in the faculty lounge, almost always sinks to the lowest common denominator. Another reason for the dumbing down is acculturation. Adults impose their customs and beliefs on the young to make them more fully human. More fully human? No, rather more fully Navaho, Nigerian, Arab, Chinese, Indian, or American. Take the children’s queries and answers I have given above. True, they all come from young American boys and girls, but are we to believe that five-year-old Nigerians and Tahitians are incapable of thoughts of comparable abstraction and depth?I now turn to a personal issue. Because I live alone and have done so all my adult life, friends try to persuade me to keep a pet to assuage my loneliness. I see their point. But a dog or a cat, endearing and intelligent as it can be, will not do. I put a pup in bed and it licks my hand. I put my young son (suppose that I have one) in bed and he gives me a hug. What’s difference? The difference is that only the human child will pop me the question, quite out of the blue, “Daddy, since I have two eyes, why don’t I see two of you?” The cure for my loneliness, then, cannot just be affection, it also has to offer me something more, which is the mystery that the universe–this dark, hulking, and irrational immensity–can produce a being so tender and vulnerable who is yet able to introduce, on the verge of sleep and oblivion itself, a shaft of light in the form of a baffling question.",
    "time": "2011"
  },
  {
    "id": 5,
    "title": "Madison, Wisconsin",
    "content": "Surely Madison is one of the most beautiful and livable mid-sized cities in the world? I feel I speak with some authority for I have lived in many cities, small and large, in a long life and have fond memories of Berkeley, California, Bloomington, Indiana, among the small, and Chongqing, Sydney, and Chicago among the large. Perhaps I am biased in favor of Madison simply because it is my home for almost thirty years, exceeding by far my stay in any other city. But this cannot be the whole answer, for one can fall in love with a city as one can with a human being at first sight and the love remains even after departure–and perhaps even especially after departure, for then memory sets to work, embroidering the ordinary into the extraordinary, and the merely pretty into the beautiful.Objectively, Madison has a claim to beauty. It is on an isthmus between two lakes, Mendota and Monona. That’s nature’s contribution. Architecturally, it is anchored at one end by Bascom Hall, the University’s administrative center, and at the other end by the State Capitol, the one a colonnaded building that exudes academic probity and weight, the other a soaring dome that exudes State power. The two buildings are linked by State Street, a student haunt filled with eat places, cafes, book stores, and clothing stores, mostly locally owned and operated, that in the regular academic semester swarm with clients at all hours, from the first break of daylight to the evening when the East Campus clock reads ten and later.Madison is a city of extremes. That’s another reason why I like it, for I am temperamentally a romantic, one with a taste for extremes of nature and culture. Let me explain what I mean. Of course, Madison’s climate is extreme, bitterly cold in winter, torridly steamy in summer, a scene of ice and snow under a pale blue sky in January and another altogether of sailboats drifting lazily on Lake Mendota and students loafing with jugs of beer on the terrace of Memorial Union in July. Culturally? What do I mean by extremes of culture? Well, here is an example. I encounter bare-foot children smiling triumphantly with tadpoles swimming in their glass jars–a picture straight out of Mark Twain, but only a stone-throw away (well, maybe a couple of stone-throws away) is one of the greatest university libraries in the world, a reminder that Madison may be only a mid-sized town, but its intellectual reach is urbi et orbi.Lastly, Madison is home for me. Home implies comfortable and nurturing routine, which is what I have established in my apartment, going every day from bedroom and kitchen to living room with its shelves of books and videos and returning to kitchen and bedroom at the end of the day. But downtown Madison is my home in the same sense–it is my home writ large. Everyday I walk from my apartment on one side of the isthmus to Science Hall on the other side, a walk through State Street that takes me about twenty-five minutes. Friends ask, Why do you do that? I answer, But you do that too, the difference being that the corridor you traverse from bedroom to study and then back again is short. My corridor, for the same purpose of transforming myself back and forth from biological to cultural being, is long, being the length of State Street.",
    "time": "2012"
  },
  {
    "id": 6,
    "title": "On Walking",
    "content": "Walking, so far as I am aware, is not highly valued in Chinese culture. It was and is seen as a means of getting from A to B. In the old days, officials were carried in sedan chairs, or they rode horses if they were of a military bent. My father was chief of protocol at the Chinese foreign ministry in the late 1930s. His job was to escort the newly appointed foreign ambassador to the residence of the Chinese president where the ambassador presented his credentials and members of his staff. The Chinese president, Lin Seng, lived on a mountain, the only access to his residence being a narrow, winding path. Father arranged for coolies and sedan chairs to carry the dignitaries up the mountain. Alas, the Soviet ambassador refused to ride on the shoulders of human beings. He insisted on walking, so father and other Chinese officials also had to walk. I can imagine them sweating and swearing under their breath at the uncouthness of the barbarians.The Soviet ambassador might have thought that he owed his desire to walk to Marxist ideology. Actually, it went deeper than that. Walking is deeply embedded in Western civilization, going back all the way to the Romans. (See Timothy M. O’Sullivan, Walking in Roman Culture, 2011). To the ancient Romans, walking–how one walked–was a mark of one’s personality and social status. Slaves walked quickly; in fact, they more or less ran. Men of stature aspired to a stately pace. It would not do to be too slow, for slowness marked one as a woman or effeminate. Fair enough as etiquette, but barbarians–Spaniards, in particular–wondered, what was walking for? Why did Roman generals pace about in the battlefield? Answer: to appear high-minded and to engage in serious conversation, a habit they acquired as civilians. Porticoes were an intrinsic part of Roman domestic architecture, its purpose being to provide shade for walking.Before the Romans, Athenians also valued walking–but only insofar as it promoted thinking. A whole school of thought, Aristotle’s “Peripatetic” school, is named after walking (peripatein–“walking around.”) Characteristically, the Greek sage, Thales, while wandering about lost in his own thoughts, fell into a well. To the Greeks, thinking effectively was more important than walking as such, the latter being merely the means, the former the end. To the Romans, walking had value in itself. The elites of modern Europe are more Roman than Greek. I am struck by how they love to walk. Perhaps I should limit my observation to just members of the English governing class who cover 10 to 20 miles of country road every day as a matter of course. For what purpose? They would have found the question impertinent, though, if pressed, they might say that it is a mark of character and builds character. And if pressed further, they might say that the British empire was as much won on the well-trodden English country roads as on the playing fields of Eton.To the true devotee, walking not only unclogs thought, it also unclogs body fluids and so promotes physical well-being. The Danish philosopher Soren Kierkegaard certainly thought so. In a letter to a friend, dated 1847, he wrote: “Above all, do not lose your desire to walk: every day I walk myself into a state of well-being and walk away from every illness; I have walked myself into my best thoughts, and I know of no thought so burdensome that one cannot walk away from it. On the other hand, the more one sits still, the closer one comes to feeling ill. Thus if one just keeps walking, everything will be all right.”All meditative practices recommend, however, the opposite. The guru sits cross-legged, perfectly still. Buddha is invariably portrayed as seated, eyes closed, immobile. And yet his last words to his disciples were “Walk on!”",
    "time": "2012"
  },
  {
    "id": 7,
    "title": "Rationalization",
    "content": "We like to think of ourselves as rational animals, which isn’t saying much, for all animals are rational; that is to say, all animals use their intelligence to achieve a rational end, namely, survival. We humans, although we too strive to survive, have other rational ends in mind, outstandingly, the acquisition of knowledge and the improvement of society. In the last two to three centuries, much is achieved in the acquisition of knowledge, but the improvement of society lags behind and is certainly less progressive. Why? The answer is that we humans, unlike other animals, not only reason but also rationalize, and that we rationalize much more in the social sphere than in the sphere of learning. How come this strong tendency to rationalize in the social sphere? At a personal and individual level, I am impelled to do so because I need to maintain an image of myself that has communal approval. Let’s say that I abuse my students. Rather than recognize the fact, which my intelligence is fully competent to do, I use it either to build a wall around that activity or persuade myself that it isn’t really abuse but is rather a form of tough love. To give another example, let’s say that I am lazy, a condition I know society disapproves of. To retain a good image of myself both in my own eyes and in the regard of society as a whole, I blame my laziness on my school’s failure to provide me with stimulus and inspiration. The blame is couched in “reasons,” but are in fact rationalizations that do not bear critical examination. If the individual is strongly tempted to rationalize in defense of its self-image, even more so is this true of the group, which requires a group self-image of distinction and superiority in order to bind its members. Unfortunately, such image-making and its purpose of binding members is all too frequently achieved, using dubious facts and specious arguments, at the expense of another group–the outsiders. The result, whether at the individual or at the group level, is antagonism and conflict that can be extremely difficult to eradicate because they are rooted in a history of rationalizations. As someone wisely said, “That which has not been reasoned into cannot be reasoned out of.” These are generalities. Let’s take a more concrete case. In the last sixty years, Americans have come to realize the gross injustice of racial discrimination, and particularly discrimination against blacks, many of whose ancestors came as slaves. To gain equality for blacks, a number of basic political rights–the right to vote, to seek office, and to form caucuses–were legislated and varyingly enforced. On this, the political front, there has been measurable success. Reason was applied to correct an egregious wrong, and it proved to be effective. Unfortunately, social equality between whites and blacks still eludes. No doubt many factors enter into play for this intractable wrong. One that, to my knowledge, has not been explored is rationalization. Both white liberals and black political leaders rationalize, rather than use reason, when they put the blame for social inequality–the lack of respect for African-Americans--almost entirely on white prejudice. But if white prejudice is so widespread and profound, how is the political gain of African-Americans ever possible?   Ironically, the political gain may have actually retarded the achievement of social equality. Why? Because if the political route is so successful, why try any other? What other routes do I have in mind? Let my answer take the form of a dream scenario. Suppose blacks, from the end of the Civil War onward, used their limited resources not on politics, but on education. A hundred and fifty years later, isn’t it possible that the best schools, the best teachers, professors, and scientists are predominantly African-American? And won’t African-Americans as a people then enjoy a social status that black governors, congressional representatives, corporate CEOs, and even a black president have not been able to confer? Another advantage of this dream-scenario is this. If African-Americans are leaders in education, if they make intellectual achievement a primary goal, then all Americans, whatever their ethnicity–Native American, Mexican, or Hmong--will want to follow suit. Certainly whites will not want to fall behind on the ground that being good in science and a nerd makes them “black” and hence unacceptable to their white peers. Sad to say, the actual scenario is the opposite of the dream one. In the actual scenario, black leaders, with the encouragement of well-meaning whites, have opted for the political route. The result of their efforts is less than what one might expect from the immense expenditure of resources: blacks, for all their political and economic gains, still lack a firm feeling of self-worth that lies at the core of equality and, hence, at the core of well-being. Morever, resorting to politics as the cure of fundamental social and racial injustice is not only ineffective, it risks bringing the ideological, rationalizing bent of politics into other areas of worthy endeavor, such as the arts, the sciences, and learning generally. So far, in the United States, the physical sciences have been largely free of political contamination. Hence, their continuing, remarkable progress. But, one wonders, for how long?  We humans are sometimes advised to emulate animals. I am skeptical of this advice, but maybe one animal quality does merit our emulation: animals do not rationalize.",
    "time": "2012"
  }
]